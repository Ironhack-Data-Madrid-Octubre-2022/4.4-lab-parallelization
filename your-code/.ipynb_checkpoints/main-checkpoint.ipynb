{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Parallelization-Lab\" data-toc-modified-id=\"Parallelization-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Parallelization Lab</a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Use-the-requests-library-to-retrieve-the-content-from-the-URL-below.\" data-toc-modified-id=\"Step-1:-Use-the-requests-library-to-retrieve-the-content-from-the-URL-below.-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Step 1: Use the requests library to retrieve the content from the URL below.</a></span></li><li><span><a href=\"#Step-2:-Use-BeautifulSoup-to-extract-a-list-of-all-the-unique-links-on-the-page.\" data-toc-modified-id=\"Step-2:-Use-BeautifulSoup-to-extract-a-list-of-all-the-unique-links-on-the-page.-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Step 2: Use BeautifulSoup to extract a list of all the unique links on the page.</a></span></li><li><span><a href=\"#Step-3:-Use-list-comprehensions-with-conditions-to-clean-the-link-list.\" data-toc-modified-id=\"Step-3:-Use-list-comprehensions-with-conditions-to-clean-the-link-list.-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Step 3: Use list comprehensions with conditions to clean the link list.</a></span></li><li><span><a href=\"#Step-4:-Use-the-os-library-to-create-a-folder-called-wikipedia-and-make-that-the-current-working-directory.\" data-toc-modified-id=\"Step-4:-Use-the-os-library-to-create-a-folder-called-wikipedia-and-make-that-the-current-working-directory.-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Step 4: Use the os library to create a folder called <em>wikipedia</em> and make that the current working directory.</a></span></li><li><span><a href=\"#Step-5:-Write-a-function-called-index_page-that-accepts-a-link-and-does-the-following.\" data-toc-modified-id=\"Step-5:-Write-a-function-called-index_page-that-accepts-a-link-and-does-the-following.-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Step 5: Write a function called index_page that accepts a link and does the following.</a></span></li><li><span><a href=\"#Step-6:-Sequentially-loop-through-the-list-of-links,-running-the-index_page-function-each-time.\" data-toc-modified-id=\"Step-6:-Sequentially-loop-through-the-list-of-links,-running-the-index_page-function-each-time.-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Step 6: Sequentially loop through the list of links, running the index_page function each time.</a></span></li><li><span><a href=\"#Step-7:-Perform-the-page-indexing-in-parallel-and-note-the-difference-in-performance.\" data-toc-modified-id=\"Step-7:-Perform-the-page-indexing-in-parallel-and-note-the-difference-in-performance.-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Step 7: Perform the page indexing in parallel and note the difference in performance.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization Lab\n",
    "\n",
    "In this lab, you will be leveraging several concepts you have learned to obtain a list of links from a web page and crawl and index the pages referenced by those links - both sequentially and in parallel. Follow the steps below to complete the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use the requests library to retrieve the content from the URL below.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Data_science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Data_science'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html=req.get(url).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use BeautifulSoup to extract a list of all the unique links on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sopa=bs(html, 'html.parser')\n",
    "\n",
    "links=[]\n",
    "for a in sopa.find_all('a'):\n",
    "    for attr in a.attrs:\n",
    "        if attr == 'href':\n",
    "            links.append(a[attr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Use list comprehensions with conditions to clean the link list.\n",
    "\n",
    "There are two types of links, absolute and relative. Absolute links have the full URL and begin with http while relative links begin with a forward slash (/) and point to an internal page within the wikipedia.org domain. Clean the respective types of URLs as follows.\n",
    "\n",
    "- Absolute Links: Create a list of these and remove any that contain a percentage sign (%).\n",
    "- Relativel Links: Create a list of these, add the domain to the link so that you have the full URL, and remove any that contain a percentage sign (%).\n",
    "- Combine the list of absolute and relative links and ensure there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlac=[e if 'https' in e else ('https://en.wikipedia.org'+e if '/wiki/' in e else '') for e in links ]\n",
    "enlaces=[i if \"%\" not in i else '' for i in enlac]\n",
    "enlaces2=[i if \".org//\" not in i else (i.replace(\".org//\",\".org/\") if \".org//\" in i else '') for i in enlaces]\n",
    "enl=[i if enlaces2.count(i) == 1 else '' for i in enlaces2]\n",
    "enl = list(filter(None, enl))\n",
    "len(enl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enlac=[]\n",
    "for e in links:\n",
    "    if 'https' in e:\n",
    "        enlac.append(e)\n",
    "    elif '/wiki/' in e:\n",
    "        enlac.append('https://en.wikipedia.org'+e)\n",
    "\n",
    "enlaces=[]\n",
    "for i in enlac:\n",
    "    if \"%\" not in i:\n",
    "        enlaces.append(i)\n",
    "\n",
    "enlaces2=[]\n",
    "for i in enlaces:\n",
    "    if \".org//\" not in i:\n",
    "        enlaces2.append(i)    \n",
    "    elif \".org//\" in i:\n",
    "        enlaces2.append(i.replace(\".org//\",\".org/\"))\n",
    "     \n",
    "\n",
    "enl=[]\n",
    "for i in enlaces2:\n",
    "    if enlaces2.count(i) == 1:\n",
    "        enl.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use the os library to create a folder called *wikipedia* and make that the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] No se puede crear un archivo que ya existe: '../your-code\\\\wikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m dir_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../your-code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_, directory)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Clase\\lib\\os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] No se puede crear un archivo que ya existe: '../your-code\\\\wikipedia'"
     ]
    }
   ],
   "source": [
    "directory = \"wikipedia\"\n",
    "dir_ = \"../your-code\"\n",
    "  \n",
    "path = os.path.join(dir_, directory)\n",
    "  \n",
    "os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\Nonito\\Desktop\\Data analysis\\apuntes_clase\\semana_4\\4.4-lab-parallelization\\your-code\\wikipedia\n"
     ]
    }
   ],
   "source": [
    "print(\"Current working directory: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Write a function called index_page that accepts a link and does the following.\n",
    "\n",
    "- Tries to request the content of the page referenced by that link.\n",
    "- Slugifies the filename using the `slugify` function from the [python-slugify](https://pypi.org/project/python-slugify/) library and adds a .html file extension.\n",
    "    - If you don't already have the python-slugify library installed, you can pip install it as follows: `$ pip install python-slugify`.\n",
    "    - To import the slugify function, you would do the following: `from slugify import slugify`.\n",
    "    - You can then slugify a link as follows `slugify(link)`.\n",
    "- Creates a file in the wikipedia folder using the slugified filename and writes the contents of the page to the file.\n",
    "- If an exception occurs during the process above, just `pass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-slugify in c:\\users\\nonito\\anaconda3\\envs\\clase\\lib\\site-packages (6.1.2)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\nonito\\anaconda3\\envs\\clase\\lib\\site-packages (from python-slugify) (1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from slugify import slugify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "folder = ''\n",
    "file_name = 'wikipedia.txt'\n",
    "file_path = os.path.join(folder, file_name)\n",
    "\n",
    "\n",
    "def index_page(s):\n",
    "    html=req.get(s).text\n",
    "    sopa=bs(html, 'html.parser')\n",
    "    slu=slugify(str(sopa))\n",
    "    try:\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(slu)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s='https://www.wikipedia.org/'\n",
    "index_page(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikipedia.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Sequentially loop through the list of links, running the index_page function each time.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:7\u001b[0m\n",
      "Cell \u001b[1;32mIn [14], line 10\u001b[0m, in \u001b[0;36mindex_page\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      8\u001b[0m html\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mget(s)\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m      9\u001b[0m sopa\u001b[38;5;241m=\u001b[39mbs(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m slu\u001b[38;5;241m=\u001b[39m\u001b[43mslugify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msopa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Clase\\lib\\site-packages\\slugify\\slugify.py:149\u001b[0m, in \u001b[0;36mslugify\u001b[1;34m(text, entities, decimal, hexadecimal, max_length, word_boundary, separator, save_order, stopwords, regex_pattern, lowercase, replacements, allow_unicode)\u001b[0m\n\u001b[0;32m    147\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m regex_pattern \u001b[38;5;129;01mor\u001b[39;00m DISALLOWED_UNICODE_CHARS_PATTERN\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m regex_pattern \u001b[38;5;129;01mor\u001b[39;00m DISALLOWED_CHARS_PATTERN\n\u001b[0;32m    151\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, DEFAULT_SEPARATOR, text)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# remove redundant\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder = ''\n",
    "file_name = 'links.txt'\n",
    "file_path = os.path.join(folder, file_name)\n",
    "\n",
    "\n",
    "for link in enl:\n",
    "    index_page(link)\n",
    "\n",
    "#tanto tiempo que da error: \"ConnectionError: HTTPSConnectionPool(host='web.archive.org', port=443): Max retries exceeded with url: /web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000023EC2983CA0>: Failed to establish a new connection: [WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Perform the page indexing in parallel and note the difference in performance.\n",
    "\n",
    "Remember to include `%%time` at the beginning of the cell so that it measures the time it takes for the cell to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 52.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder = ''\n",
    "file_name = 'linksmp.txt'\n",
    "file_path = os.path.join(folder, file_name)\n",
    "\n",
    "pool=mp.Pool(mp.cpu_count())\n",
    "\n",
    "resul=pool.map_async(index_page, enl).get\n",
    "\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(str(resul))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ApplyResult.get of <multiprocessing.pool.MapResult object at 0x0000014BD9C56250>>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
